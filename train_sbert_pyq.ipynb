{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71853add",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyq_pairs = [\n",
    "    # POSITIVE PAIRS (label = 1.0) – same or very similar question\n",
    "    (\"Explain basic COCOMO effort estimation model.\",\n",
    "     \"Discuss COCOMO effort prediction model in detail.\", 1.0),\n",
    "\n",
    "    (\"Explain use case diagrams with example.\",\n",
    "     \"Describe UML use case diagram.\", 1.0),\n",
    "\n",
    "    (\"What is SRS? Explain characteristics of a good SRS.\",\n",
    "     \"List qualities of a good software requirements specification.\", 1.0),\n",
    "\n",
    "    # NEGATIVE PAIRS (label = 0.0) – clearly different topics\n",
    "    (\"Explain basic COCOMO effort estimation model.\",\n",
    "     \"Draw a use case diagram for library management system.\", 0.0),\n",
    "\n",
    "    (\"Explain black box testing.\",\n",
    "     \"What is spiral model? Explain phases.\", 0.0),\n",
    "\n",
    "    (\"Discuss various requirement elicitation techniques.\",\n",
    "     \"Explain deadlock and its conditions in OS.\", 0.0),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11495fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\learnova_pyq\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Please install `datasets` to use this function: `pip install datasets`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m warmup_steps = math.ceil(\u001b[38;5;28mlen\u001b[39m(train_dataloader) * num_epochs * \u001b[32m0.1\u001b[39m)\n\u001b[32m     28\u001b[39m output_dir = \u001b[33m\"\u001b[39m\u001b[33mmodels/pyq-sbert\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Fine-tuned model saved to:\u001b[39m\u001b[33m\"\u001b[39m, output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\learnova_pyq\\venv\\Lib\\site-packages\\sentence_transformers\\fit_mixin.py:246\u001b[39m, in \u001b[36mFitMixin.fit\u001b[39m\u001b[34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit, resume_from_checkpoint)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mDeprecated training method from before Sentence Transformers v3.0, it is recommended to use\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03m:class:`~sentence_transformers.trainer.SentenceTransformerTrainer` instead. This method uses\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m \u001b[33;03m        to continue training from.\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_datasets_available():\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPlease install `datasets` to use this function: `pip install datasets`.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# Delayed import to counter the SentenceTransformers -> FitMixin -> SentenceTransformerTrainer -> SentenceTransformers circular import\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerTrainer\n",
      "\u001b[31mImportError\u001b[39m: Please install `datasets` to use this function: `pip install datasets`."
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "# 1. Start from pretrained SBERT\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# 2. Convert our pairs into InputExamples\n",
    "train_examples = [\n",
    "    InputExample(\n",
    "        texts=[s1, s2],\n",
    "        label=float(score)\n",
    "    )\n",
    "    for (s1, s2, score) in pyq_pairs\n",
    "]\n",
    "\n",
    "# 3. DataLoader\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8)\n",
    "\n",
    "# 4. Loss function – cosine similarity loss for sentence similarity\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# 5. Fine-tune for a few epochs\n",
    "num_epochs = 2   # keep it small to avoid overfitting\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)\n",
    "\n",
    "output_dir = \"models/pyq-sbert\"\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    show_progress_bar=True,\n",
    "    output_path=output_dir,\n",
    ")\n",
    "\n",
    "print(\"✅ Fine-tuned model saved to:\", output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
